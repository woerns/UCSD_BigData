{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Batch learning, Online learning and regularization ##\n",
      "In the previous lessons we have learned about two types of statistical inference problem that can be stated as minimization of some **loss function**. The loss function measures the degree of fit between the parameters of the statistical model, which we denote by $\\theta$, and the data, which we denote by $X$.\n",
      "\n",
      "#### Previous Examples ####\n",
      "* **Mean:** Find the vector $\\mu$ such that $\\Phi_{\\mu}(\\mu,X) = \\frac{1}{n} \\sum_{i=1}^n \\|\\mu - x_i\\|_2^2$ is minimized.\n",
      "* **Largest Principle Component:** Find the unit length eigen-vector $v, \\|v\\|_2 = 1$ \n",
      "that minimized $\\Phi_{PC1}(v,\\Sigma) \\doteq  -v^\\top \\Sigma v$\n",
      "where $\\Sigma \\doteq \\frac{1}{n} \\sum_{i=1}^n (x_i-\\mu) (x_i-\\mu)^{\\top}$.   \n",
      "Equivalently, maximize the square length of the projection of the data on $v$, which corresponds to **minimizing** the loss function:\n",
      "$$\\Phi_{PC1}(v,\\Sigma) = - \\frac{1}{n} \\sum_{i=1}^n v^\\top (x_i-\\mu) (x_i-\\mu)^{\\top} v \n",
      "= - \\frac{1}{n} \\sum_{i=1}^n (v^\\top (x_i-\\mu))^2$$\n",
      "* **Linear Regression:**\n",
      "#### New Examples ####\n",
      "* **Logistic Regression**\n",
      "* **Linear Discriminator**\n",
      "#### Regularization ####\n",
      "\n",
      "**Batch** methods for solving these problems amount to equating the gradient of the loss function to zero. \n",
      "$$\\frac{\\partial}{\\partial \\theta_i} \\Phi(\\theta,X)=0$$\n",
      "This is a good approach when data $X$ is small enough to fit in memory. When we have more data than can fit in memory we need to use something else.\n",
      "\n",
      "One powerful solution is **regularized stocahstic gradient descent**. The power of this approach is in that the estimated parameter $\\hat{\\theta}$ is updated sequentially and each update depends only on a single example. As a result, implementing these examples requires only a small amount of memory, data is read into memory in blocks and discarded when the following block is read in."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## A general scheme for online minimization ##\n",
      "\n",
      "Balancing error minimization with minimization of change."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## LMS - Least Mean Square (Widrow-Hoff)##"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Least squares using exponential weights  ##"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## RLS  - Recursive Least Squares ##"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Scaling of the learning rate with time ##"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}